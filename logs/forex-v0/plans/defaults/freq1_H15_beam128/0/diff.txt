diff --git a/Trajectory_Transformer/config/offline.py b/Trajectory_Transformer/config/offline.py
index 1dd7eb6..98c4875 100644
--- a/Trajectory_Transformer/config/offline.py
+++ b/Trajectory_Transformer/config/offline.py
@@ -17,7 +17,7 @@ args_to_watch = [
 base = {
 
     'train': {
-        'N': 100,
+        'N': 20,
         'discount': 0.99,
         'n_layer': 4,
         'n_head': 4,
diff --git a/Trajectory_Transformer/scripts/plan.py b/Trajectory_Transformer/scripts/plan.py
index 881688c..f06654a 100644
--- a/Trajectory_Transformer/scripts/plan.py
+++ b/Trajectory_Transformer/scripts/plan.py
@@ -1,10 +1,15 @@
 import json
 import pdb
+import os
+import sys
 from os.path import join
 import gym
 import gym_anytrading
+import numpy as np
 from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions 
 from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL
+parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
+sys.path.insert(0, parent_dir)
 import trajectory.utils as utils
 import trajectory.datasets as datasets
 from trajectory.search import (
@@ -39,7 +44,7 @@ gpt, gpt_epoch = utils.load_model(args.logbase, args.dataset, args.gpt_loadpath,
 #######################
 
 env = gym.make(args.dataset, frame_bound=(50, 100), window_size=10)
-renderer = utils.make_renderer(args)
+#renderer = utils.make_renderer(args)
 timer = utils.timer.Timer()
 
 discretizer = dataset.discretizer
@@ -48,7 +53,7 @@ observation_dim = dataset.observation_dim
 action_dim = dataset.action_dim
 
 value_fn = lambda x: discretizer.value_fn(x, args.percentile)
-preprocess_fn = datasets.get_preprocess_fn(env.name)
+#preprocess_fn = datasets.get_preprocess_fn(env.name)
 
 #######################
 ###### main loop ######
@@ -63,10 +68,11 @@ rollout = [observation.copy()]
 ## previous (tokenized) transitions for conditioning transformer
 context = []
 
-T = env.max_episode_steps
+T = 1000000
 for t in range(T):
 
-    observation = preprocess_fn(observation)
+    #observation = preprocess_fn(observation)
+    observation = observation.reshape(-1)
 
     if t % args.plan_freq == 0:
         ## concatenate previous transitions and current observations to input to model
@@ -90,18 +96,18 @@ for t in range(T):
     action = extract_actions(sequence_recon, observation_dim, action_dim, t=0)
 
     ## execute action in environment
-    next_observation, reward, terminal, _ = env.step(action)
+    next_observation, reward, terminal, info = env.step(np.argmax(action))
 
     ## update return
     total_reward += reward
-    score = env.get_normalized_score(total_reward)
+    #score = env.get_normalized_score(total_reward)
 
     ## update rollout observations and context transitions
     rollout.append(next_observation.copy())
     context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
 
     print(
-        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} | score: {score:.4f} | '
+        f'[ plan ] t: {t} / {T} | r: {reward:.2f} | R: {total_reward:.2f} '
         f'time: {timer():.2f} | {args.dataset} | {args.exp_name} | {args.suffix}\n'
     )
 
@@ -114,11 +120,13 @@ for t in range(T):
     #    ## save rollout thus far
     #    renderer.render_rollout(join(args.savepath, f'rollout.mp4'), rollout, fps=80)
 
-    if terminal: break
+    if terminal: 
+        print(info)
+        break
 
     observation = next_observation
 
 ## save result as a json file
 json_path = join(args.savepath, 'rollout.json')
-json_data = {'score': score, 'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
+json_data = {'step': t, 'return': total_reward, 'term': terminal, 'gpt_epoch': gpt_epoch}
 json.dump(json_data, open(json_path, 'w'), indent=2, sort_keys=True)
diff --git a/Trajectory_Transformer/scripts/train.py b/Trajectory_Transformer/scripts/train.py
index ddcda7a..9a2273b 100644
--- a/Trajectory_Transformer/scripts/train.py
+++ b/Trajectory_Transformer/scripts/train.py
@@ -2,11 +2,17 @@ import os
 import numpy as np
 import torch
 import pdb
+import sys
+
+parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
+sys.path.insert(0, parent_dir)
 
 import trajectory.utils as utils
 import trajectory.datasets as datasets
 from trajectory.models.transformers import GPT
 
+os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
+os.environ["CUDA_VISIBLE_DEVICES"] = '2'
 
 class Parser(utils.Parser):
     dataset: str = 'forex-v0'
@@ -31,7 +37,7 @@ dataset_config = utils.Config(
     savepath=(args.savepath, 'data_config.pkl'),
     env=args.dataset,
     N=args.N,
-    penalty=args.termination_penalty,
+    penalty=None,
     sequence_length=sequence_length,
     step=args.step,
     discount=args.discount,
@@ -104,7 +110,8 @@ trainer = trainer_config()
 #######################
 
 ## scale number of epochs to keep number of updates constant
-n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+#n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
+n_epochs = 3000
 save_freq = int(n_epochs // args.n_saves)
 
 for epoch in range(n_epochs):
diff --git a/Trajectory_Transformer/trajectory/datasets/Random/forex-v0_random.pkl b/Trajectory_Transformer/trajectory/datasets/Random/forex-v0_random.pkl
deleted file mode 100644
index fa97c75..0000000
Binary files a/Trajectory_Transformer/trajectory/datasets/Random/forex-v0_random.pkl and /dev/null differ
diff --git a/Trajectory_Transformer/trajectory/datasets/Random/gen_data.py b/Trajectory_Transformer/trajectory/datasets/Random/gen_data.py
index 71bfb7e..bbd08e4 100644
--- a/Trajectory_Transformer/trajectory/datasets/Random/gen_data.py
+++ b/Trajectory_Transformer/trajectory/datasets/Random/gen_data.py
@@ -13,7 +13,7 @@ env = gym.make('forex-v0', frame_bound=(50, 100), window_size=10)
 action_dim = env.action_space.n
 
 episode = 10
-
+T = 0
 episode_data = {}
 for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:
     episode_data[k] = []
@@ -25,13 +25,12 @@ for _ in range(episode):
         episode_data['observations'].append(observation.reshape(-1).astype('float32'))
         episode_data['next_observations'].append(next_observation.reshape(-1).astype('float32'))
         episode_data['actions'].append(action)
-        episode_data['rewards'].append(np.array(reward).astype('float32'))
+        episode_data['rewards'].append(np.array([reward]).astype('float32'))
         episode_data['terminals'].append(done)
         if done:
             break
 
 for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:
     episode_data[k] = np.stack(episode_data[k])
-
-with open('trajectory-transformer/trajectory/datasets/Random/{}_'.format(quat_type) + '_random.pkl', 'wb') as f:
+with open('/home/kjlin0508/Course_work/AI_Intro/RL_for_Quatitatitive_Trading/Trajectory_Transformer/trajectory/datasets/Random/{}_r'.format(quat_type) + '.pkl', 'wb') as f:
     pickle.dump(episode_data, f)
\ No newline at end of file
diff --git a/Trajectory_Transformer/trajectory/datasets/__init__.py b/Trajectory_Transformer/trajectory/datasets/__init__.py
index 69ee58d..d1062c5 100644
--- a/Trajectory_Transformer/trajectory/datasets/__init__.py
+++ b/Trajectory_Transformer/trajectory/datasets/__init__.py
@@ -1,3 +1,3 @@
-from .d4rl import load_environment
+#from .d4rl import load_environment
 from .sequence import *
 from .preprocessing import get_preprocess_fn
diff --git a/Trajectory_Transformer/trajectory/datasets/sequence.py b/Trajectory_Transformer/trajectory/datasets/sequence.py
index c23b4f3..4525194 100644
--- a/Trajectory_Transformer/trajectory/datasets/sequence.py
+++ b/Trajectory_Transformer/trajectory/datasets/sequence.py
@@ -53,7 +53,7 @@ class SequenceDataset(torch.utils.data.Dataset):
         self.device = device
         
         print(f'[ datasets/sequence ] Loading...', end=' ', flush=True)
-        with open('trajectory-transformer/trajectory/datasets/Random'+env+'.pkl', 'rb') as f:
+        with open('Trajectory_Transformer/trajectory/datasets/Random/'+env+'.pkl', 'rb') as f:
             dataset = pickle.load(f)
         print('âœ“')
 
@@ -69,7 +69,6 @@ class SequenceDataset(torch.utils.data.Dataset):
         terminals = dataset['terminals']
         realterminals = [False]*len(dataset['terminals'])
 
-        #observations = np.reshape(observations, (100, 7000))
         self.observations_raw = observations
         self.actions_raw = actions
         self.next_observations_raw = next_observations
diff --git a/Trajectory_Transformer/trajectory/utils/__init__.py b/Trajectory_Transformer/trajectory/utils/__init__.py
index 7c596c3..7529384 100644
--- a/Trajectory_Transformer/trajectory/utils/__init__.py
+++ b/Trajectory_Transformer/trajectory/utils/__init__.py
@@ -2,7 +2,7 @@ from .setup import Parser, watch
 from .arrays import *
 from .serialization import *
 from .progress import Progress, Silent
-from .rendering import make_renderer
+#from .rendering import make_renderer
 # from .video import *
 from .config import Config
 from .training import Trainer
diff --git a/requirements.txt b/requirements.txt
index ece16ed..a579177 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,14 +1,16 @@
 numpy
 gym
 numpy
-torch
+pytorch==1.12.1
+torchvision==0.13.1 
+torchaudio==0.12.1
 transformers==4.5.1
 wandb==0.9.1
 tensorboard
 pyprind
 tensorflow
 gin-config
-gym
+gym==0.21.0
 tqdm
 blosc
 git+https://github.com/google/dopamine.git