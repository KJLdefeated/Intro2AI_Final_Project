diff --git a/DDQN.py b/DDQN.py
index 2e34be3..6b01dd9 100644
--- a/DDQN.py
+++ b/DDQN.py
@@ -277,6 +277,37 @@ def test(env):
     print(env._total_profit)
     print(env._total_reward)
 
+def state_preprocess(state):
+    tempstate = state
+    for i in range(12):
+        for j in range(4):
+            tempstate[i*4+j] = (state[44+j] - state[i*4+j])/state[44+j]
+    return tempstate
+
+def gen_offline_data(episodes, env):
+    agent = Agent(env)
+    agent.target_net.load_state_dict(torch.load("./Tables/DDQN.pt"))
+    episode_data = {}
+    for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:
+        episode_data[k] = []
+    for _ in range(episodes):
+        observation = env.reset()
+        observation = state_preprocess(observation.reshape(-1))
+        while True:
+            Q = agent.target_net.forward(torch.FloatTensor(observation)).squeeze(0).detach()
+            action = int(torch.argmax(Q).numpy())
+            next_observation, reward, done, info = env.step(action)
+            episode_data['observations'].append(np.array(observation))
+            next_observation = state_preprocess(next_observation.reshape(-1))
+            episode_data['next_observations'].append(np.array(next_observation))
+            episode_data['actions'].append(np.array(Q))
+            episode_data['rewards'].append(np.array([reward]).astype('float32'))
+            episode_data['terminals'].append(np.array([done]))
+            observation = next_observation
+            if done:
+                print(info)
+                break
+    return episode_data
 
 if __name__ == "__main__":
     env = buildEnv.createEnv(2330)        
diff --git a/Trajectory_Transformer/config/offline.py b/Trajectory_Transformer/config/offline.py
index 73c5b28..a05b7cd 100644
--- a/Trajectory_Transformer/config/offline.py
+++ b/Trajectory_Transformer/config/offline.py
@@ -17,7 +17,7 @@ args_to_watch = [
 base = {
 
     'train': {
-        'N': 20,
+        'N': 100,
         'discount': 0.99,
         'n_layer': 4,
         'n_head': 4,
@@ -58,7 +58,7 @@ base = {
 
         'plan_freq': 1,
         'horizon': 10,
-        'beam_width': 128,
+        'beam_width': 32,
         'n_expand': 2,
 
         'k_obs': 1,
diff --git a/Trajectory_Transformer/scripts/plan.py b/Trajectory_Transformer/scripts/plan.py
index 8979c32..0b5cd38 100644
--- a/Trajectory_Transformer/scripts/plan.py
+++ b/Trajectory_Transformer/scripts/plan.py
@@ -20,11 +20,11 @@ from trajectory.search import (
     update_context,
 )
 os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
-os.environ["CUDA_VISIBLE_DEVICES"] = '3'
+os.environ["CUDA_VISIBLE_DEVICES"] = '0'
 
 code = '2330'
 class Parser(utils.Parser):
-    dataset: str = 'stock_'+code
+    dataset: str = 'DDQN_1_2330'
     config: str = 'config.offline'
 
 #######################
@@ -67,17 +67,28 @@ value_fn = lambda x: discretizer.value_fn(x, args.percentile)
 observation = env.reset()
 total_reward = 0
 
+def state_preprocess(state):
+    tempstate = state
+    for i in range(12):
+        for j in range(4):
+            tempstate[i*4+j] = (state[44+j] - state[i*4+j])/state[44+j]
+    return tempstate
 ## observations for rendering
+observation = observation.reshape(-1)
+observation = state_preprocess(observation)
 rollout = [observation.copy()]
 
 ## previous (tokenized) transitions for conditioning transformer
 context = []
 
+
+
 T = 1187
 for t in range(T):
 
     #observation = preprocess_fn(observation)
-    observation = observation.reshape(-1)
+    #observation = observation.reshape(-1)
+    #observation = state_preprocess(observation)
 
     if t % args.plan_freq == 0:
         ## concatenate previous transitions and current observations to input to model
@@ -108,6 +119,7 @@ for t in range(T):
     #score = env.get_normalized_score(total_reward)
 
     ## update rollout observations and context transitions
+    next_observation = state_preprocess(next_observation.reshape(-1))
     rollout.append(next_observation.copy())
     context = update_context(context, discretizer, observation, action, reward, args.max_context_transitions)
     print(
diff --git a/Trajectory_Transformer/scripts/train.py b/Trajectory_Transformer/scripts/train.py
index bb35461..b2cfabd 100644
--- a/Trajectory_Transformer/scripts/train.py
+++ b/Trajectory_Transformer/scripts/train.py
@@ -12,10 +12,10 @@ import trajectory.datasets as datasets
 from trajectory.models.transformers import GPT
 
 os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
-os.environ["CUDA_VISIBLE_DEVICES"] = '2'
+os.environ["CUDA_VISIBLE_DEVICES"] = '0'
 
 class Parser(utils.Parser):
-    dataset: str = 'stock_2330'
+    dataset: str = 'DDQN_1_2330'
     config: str = 'config.offline'
 
 #######################
@@ -111,7 +111,7 @@ trainer = trainer_config()
 
 ## scale number of epochs to keep number of updates constant
 #n_epochs = int(1e6 / len(dataset) * args.n_epochs_ref)
-n_epochs = 10000
+n_epochs = 3000
 save_freq = int(n_epochs // args.n_saves)
 
 for epoch in range(n_epochs):
diff --git a/Trajectory_Transformer/trajectory/datasets/Medium/DDQN.py b/Trajectory_Transformer/trajectory/datasets/Medium/DDQN.py
deleted file mode 100644
index 2e34be3..0000000
--- a/Trajectory_Transformer/trajectory/datasets/Medium/DDQN.py
+++ /dev/null
@@ -1,294 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import numpy as np
-import gym
-import random
-from collections import deque
-from torch import Tensor
-import os
-from tqdm import tqdm
-import buildEnv
-import math
-import math
-total_rewards = []
-
-
-class replay_buffer():
-    '''
-    A deque storing trajectories
-    '''
-    def __init__(self, capacity):
-        self.capacity = capacity  # the size of the replay buffer
-        self.memory = deque(maxlen=capacity)  # replay buffer itself
-
-    def insert(self, state, action, reward, next_state, done):
-        '''
-        Insert a sequence of data gotten by the agent into the replay buffer.
-        Parameter:
-            state: the current state
-            action: the action done by the agent
-            reward: the reward agent got
-            next_state: the next state
-            done: the status showing whether the episode finish        
-        Return:
-            None
-        '''
-        
-        self.memory.append([state, action, reward, next_state.reshape(48), done])
-
-    def sample(self, batch_size):
-        '''
-        Sample a batch size of data from the replay buffer.
-        Parameter:
-            batch_size: the number of samples which will be propagated through the neural network
-        Returns:
-            observations: a batch size of states stored in the replay buffer
-            actions: a batch size of actions stored in the replay buffer
-            rewards: a batch size of rewards stored in the replay buffer
-            next_observations: a batch size of "next_state"s stored in the replay buffer
-            done: a batch size of done stored in the replay buffer
-        '''
-        batch = random.sample(self.memory, batch_size)
-        observations, actions, rewards, next_observations, done = zip(*batch)
-        return observations, actions, rewards, next_observations, done
-
-    def __len__(self):
-        return len(self.memory)
-
-
-class Net(nn.Module):
-    '''
-    The structure of the Neural Network calculating Q values of each state.
-    '''
-    def __init__(self,  num_actions, hidden_layer_size=600):
-        super(Net, self).__init__()
-        self.input_state = 48  # the dimension of state space
-        self.num_actions = num_actions  # the dimension of action space
-        self.fc1 = nn.Linear(self.input_state, 32*12)  # input layer
-        self.fc2 = nn.Linear(32*12, hidden_layer_size)  # hidden layer
-        self.fc3 = nn.Linear(hidden_layer_size, hidden_layer_size)
-        self.fc4 = nn.Linear(hidden_layer_size, num_actions)  # output layer
-
-    def forward(self, states):
-        '''
-        Forward the state to the neural network.        
-        Parameter:
-            states: a batch size of states
-        Return:
-            q_values: a batch size of q_values
-        '''
-        x = F.relu(self.fc1(states))
-        x = F.relu(self.fc2(x))
-        x = F.relu(self.fc3(x))
-        q_values = self.fc4(x)
-        return q_values
-
-
-class Agent():
-    def __init__(self, env, epsilon=10, learning_rate=0.0002, GAMMA=0.97, batch_size=32, capacity=10000):
-        """
-        The agent learning how to control the action of the cart pole.
-        Hyperparameters:
-            epsilon: Determines the explore/expliot rate of the agent
-            learning_rate: Determines the step size while moving toward a minimum of a loss function
-            GAMMA: the discount factor (tradeoff between immediate rewards and future rewards)
-            batch_size: the number of samples which will be propagated through the neural network
-            capacity: the size of the replay buffer/memory
-        """
-        self.env = env
-        self.n_actions = 2  # the number of actions
-        self.count = 0
-
-        self.epsilon = epsilon
-        self.learning_rate = learning_rate
-        self.gamma = GAMMA
-        self.batch_size = batch_size
-        self.capacity = capacity
-
-        self.buffer = replay_buffer(self.capacity)
-        self.evaluate_net = Net(self.n_actions)  # the evaluate network
-        self.target_net = Net(self.n_actions)  # the target network
-        self.optimizer = torch.optim.Adam(
-            self.evaluate_net.parameters(), lr=self.learning_rate)  # Adam is a method using to optimize the neural network
-
-    def learn(self):
-        '''
-        - Implement the learning function.
-        - Here are the hints to implement.
-        Steps:
-        -----
-        1. Update target net by current net every 100 times. (we have done this for you)
-        2. Sample trajectories of batch size from the replay buffer.
-        3. Forward the data to the evaluate net and the target net.
-        4. Compute the loss with MSE.
-        5. Zero-out the gradients.
-        6. Backpropagation.
-        7. Optimize the loss function.
-        -----
-        Parameters:
-            self: the agent itself.
-            (Don't pass additional parameters to the function.)
-            (All you need have been initialized in the constructor.)
-        Returns:
-            None (Don't need to return anything)
-        '''
-        if self.count % 10 == 0:
-            self.target_net.load_state_dict(self.evaluate_net.state_dict())
-
-        # Begin your code
-        # TODO
-        # Step2: Sample the data stored in the buffer and store them into data type Tensor 
-        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)
-        states = torch.tensor(np.array(states), dtype=torch.float)
-        actions = torch.tensor(np.array(actions), dtype=torch.int64).unsqueeze(-1)
-        rewards = torch.tensor(np.array(rewards), dtype=torch.float)
-        next_states = torch.tensor(np.array(next_states), dtype=torch.float)
-        dones = torch.tensor(np.array(dones), dtype=torch.float)
-
-        # Step3: Forward the data to the evaluate net and the target net with a few adjustment of the size
-        
-        q_values = torch.gather(self.evaluate_net(states), 1, actions)
-        
-        next_actions = self.evaluate_net(next_states).argmax(dim=1, keepdim=True)
-        next_q_values = self.target_net(next_states).gather(1, next_actions).reshape(32)
-        target_q_values = (rewards + self.gamma * (1 - dones) * next_q_values).unsqueeze(1)
-        # Step4: Compute the loss with MSE.
-        loss = F.mse_loss(q_values, target_q_values)
-        
-        # Step5: Zero-out the gradients.
-        self.optimizer.zero_grad()
-
-        # Step6: Backpropagation.
-        loss.backward()
-        # Step7: Optimize the loss function.
-        self.optimizer.step()
-        
-            
-        # End your code
-        
-
-
-    def choose_action(self, state):
-
-        with torch.no_grad():
-            # Begin your code
-            # TODO
-            temp = np.random.random()
-            if temp < math.exp(-1*self.epsilon) or temp<0.005:
-                return np.random.randint(self.n_actions)
-            # forward the state to nn and find the argmax of the actions
-            
-            action = torch.argmax(self.evaluate_net(Tensor(state).reshape(48))).item()
-            # End your code
-        return action
-
-
-def train(env):
-    """
-    Train the agent on the given environment.
-    Paramenters:
-        env: the given environment.
-    Returns:
-        None (Don't need to return anything)
-    """
-    agent = Agent(env)
-    #agent.target_net.load_state_dict(torch.load("/content/drive/My Drive/Colab/RL_for_Quatitatitive_Trading/Tables/DDQN3850.pt"))
-    #agent.evaluate_net.load_state_dict(torch.load("/content/drive/My Drive/Colab/RL_for_Quatitatitive_Trading/Tables/DDQN3850.pt"))
-    episode = 150
-    rewards = []
-    cnt = 0
-    for _ in tqdm(range(episode)):
-        cnt += 1
-        state = env.reset()
-        #print(state)
-        count0 = 0
-        count1 = 0
-        while True:
-            agent.count += 1
-            #env.render()
-            tempstate1 = state.reshape(48)
-            state = state.reshape(48)
-            for i in range(12):
-                for j in range(4):
-                    tempstate1[i*4+j] = (state[44+j] - state[4*i+j])/state[44+j]
-            action = agent.choose_action(tempstate1)
-            next_state, reward, done, _ = env.step(action)
-            tempstate2 = next_state.reshape(48)
-            next_state = next_state.reshape(48)
-            for i in range(12):
-                for j in range(4):
-                    tempstate2[i*4+j] = (next_state[44+j] - next_state[4*i+j])/next_state[44+j]
-            agent.buffer.insert(tempstate1, int(action), reward, tempstate2, int(done))
-            if(action==1):
-                count1 += 1
-            else:
-                count0 += 1
-            if len(agent.buffer) >= 100:
-                agent.learn()
-            if done:
-                rewards.append(env._total_reward)
-                #print("!")
-                #print(count0)
-                #print(count1)
-                #print(agent.env._total_reward)
-                #print(agent.env._total_profit)
-                break
-            state = next_state
-        agent.epsilon += 0.1
-        
-        if(cnt % 50 ==0):
-            url = "Tables/DDQN"+str(cnt+3850)+".pt"
-            url2 = "Rewards/DDQN_rewards_iter2_new"+str(cnt+3850)+".npy"
-            try:
-                np.save(url2, np.array(rewards))
-                print(".np saved at "+url2)
-            except RuntimeError:
-                print("!!")  
-            try:
-                torch.save(agent.target_net.state_dict(), url)
-            except RuntimeError:
-                print("!!!")
-
-def test(env):
-    """
-    Test the agent on the given environment.
-    Paramenters:
-        env: the given environment.
-    Returns:
-        None (Don't need to return anything)
-    """
-    testing_agent = Agent(env)
-    testing_agent.target_net.load_state_dict(torch.load("Tables/DDQN2850.pt"))
-    for _ in range(1):
-        state = env.reset().reshape(48)
-        while True:
-            tempstate = state
-            for i in range(12):
-                for j in range(4):
-                    tempstate[i*4+j] = (state[44+j] - state[i*4+j])/state[44+j]
-            Q = testing_agent.target_net(
-                torch.FloatTensor(tempstate.reshape(48))).squeeze(0).detach()
-            action = int(torch.argmax(Q).numpy())
-            next_state, _, done, _ = env.step(action)
-            if done:
-                break
-            state = next_state.reshape(48)
-    print(env._total_profit)
-    print(env._total_reward)
-
-
-if __name__ == "__main__":
-    env = buildEnv.createEnv(2330)        
-    os.makedirs("./Tables", exist_ok=True)
-    os.makedirs("./Rewards", exist_ok=True)
-    # training section:
-    for i in range(1):
-        print(f"#{i + 1} training progress")
-        #with tf.device('/device:GPU:0'):
-        #train(env)
-        
-    # testing section:
-    test(env)
-    env.close()
-    #np.save("./Rewards/DDQN_rewards.npy", np.array(total_rewards))
\ No newline at end of file
diff --git a/Trajectory_Transformer/trajectory/datasets/Medium/buildEnv.py b/Trajectory_Transformer/trajectory/datasets/Medium/buildEnv.py
deleted file mode 100644
index d788c38..0000000
--- a/Trajectory_Transformer/trajectory/datasets/Medium/buildEnv.py
+++ /dev/null
@@ -1,86 +0,0 @@
-import numpy as np
-import gym
-import gym_anytrading
-from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions
-import pandas as pd
-
-def my_process_data(env):
-    start = env.frame_bound[0] - env.window_size
-    end = env.frame_bound[1]
-
-    prices = env.df['Close'].to_numpy()
-    prices = prices[start:end]
-    signal_features = env.df.loc[:, ['Open', 'Close', 'High', 'Low']].to_numpy()[start:end]
-    return prices, signal_features
-
-def my_calculate_reward(self, action):
-    
-    '''
-    # this is the original one
-    step_reward = 0
-    trade = False
-    if ((action == Actions.Buy.value and self._position == Positions.Short) or
-        (action == Actions.Sell.value and self._position == Positions.Long)):
-        trade = True
-    '''
-    '''
-    for i in range(13,3,-2):
-        ismin, ismax = self.knowIs(i)
-        if(ismax and action == Actions.Buy.value):
-            return (i*-5 + 15)/10
-        if(ismin and action == Actions.Sell.value):
-            return (-5 * i + 15)/10
-    '''
-    '''  
-    if trade:
-        current_price = self.prices[self._current_tick]
-        last_trade_price = self.prices[self._last_trade_tick]
-        price_diff = current_price - last_trade_price
-        #step_reward = 0.5
-        #if action == Actions.Sell.value:
-        #    step_reward += price_diff
-        #if action == Actions.Sell.value and ismax:
-        #    step_reward += 3
-        #if action == Actions.Buy.value and ismin:
-        #    step_reward += 3
-        #else:
-        #    step_reward -= price_diff
-    
-    
-    
-    return step_reward
-    '''
-
-    if action == Actions.Sell.value and self._current_tick!= self._end_tick:
-        return self.prices[self._current_tick] - self.prices[self._current_tick+1]
-    if action == Actions.Buy.value and self._current_tick!= self._end_tick:
-        return self.prices[self._current_tick+1] - self.prices[self._current_tick]
-    return 0
-
-
-class MyStocksEnv(StocksEnv):
-    _process_data = my_process_data
-    _calculate_reward = my_calculate_reward
-    def knowIs(self, window):
-        ismax = False
-        ismin = False 
-        if self._current_tick < self._end_tick-window/2:
-            ismax = True
-            ismin = True
-            for i in range(int(-window/2),int(window/2+1)):
-                if(self.prices[self._current_tick + i] > self.prices[self._current_tick]):
-                    ismax = False
-                if(self.prices[self._current_tick + i] < self.prices[self._current_tick]):
-                    ismin = False
-        return ismax, ismin
-
-
-def createEnv(stock_no, window_size = 12, frame_bounds = (12, 1200)):
-    csv_name = './dataset/stock_data_' + str(stock_no) + '.csv'
-    data = pd.read_csv(csv_name)
-    read_df = pd.DataFrame(data)
-    read_df = read_df.loc[::-1].reset_index(drop=True)
-    env = MyStocksEnv(df = read_df, window_size = window_size, frame_bound = frame_bounds)
-    return env
-if __name__ == "__main__":
-    createEnv(2330)
diff --git a/Trajectory_Transformer/trajectory/datasets/Medium/gen_data.py b/Trajectory_Transformer/trajectory/datasets/Medium/gen_data.py
deleted file mode 100644
index 67bdff1..0000000
--- a/Trajectory_Transformer/trajectory/datasets/Medium/gen_data.py
+++ /dev/null
@@ -1,57 +0,0 @@
-import gym
-import gym_anytrading
-from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions 
-from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL
-import matplotlib.pyplot as plt
-import numpy as np
-import pickle
-import os
-import sys
-from buildEnv import createEnv
-from DDQN import Agent
-import torch
-from tqdm import tqdm
-
-quat_type = 2330
-env = createEnv(2330)
-# env = gym.make('stocks-v0', frame_bound=(50, 100), window_size=10)
-
-testing_agent = Agent(env)
-testing_agent.target_net.load_state_dict(torch.load("/home/kjlin0508/Course_work/AI_Intro/RL_for_Quatitatitive_Trading/Tables/DDQN.pt"))
-
-action_dim = env.action_space.n
-
-episode = 100
-T = 0
-episode_data = {}
-for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:
-    episode_data[k] = []
-for _ in tqdm(range(episode)):
-    observation = env.reset().reshape(-1)
-    while True:
-        tempstate = observation.reshape(-1)
-        for i in range(12):
-            for j in range(4):
-                tempstate[i*4+j] = (observation[44+j] - observation[i*4+j])/observation[44+j]
-        Q = testing_agent.target_net(torch.FloatTensor(tempstate.reshape(48))).squeeze(0).detach()
-        action = int(torch.argmax(Q).numpy())
-        next_observation, reward, done, _ = env.step(action)
-        observation = next_observation.reshape(48)
-        episode_data['observations'].append(tempstate.astype('float32'))
-        next_observation = next_observation.reshape(-1)
-        tempstate = next_observation
-        for i in range(12):
-            for j in range(4):
-                tempstate[i*4+j] = (next_observation[44+j] - next_observation[i*4+j])/next_observation[44+j]
-        episode_data['next_observations'].append(tempstate.astype('float32'))
-        episode_data['actions'].append(np.array(Q))
-        #print(Q)
-        episode_data['rewards'].append(np.array([reward]).astype('float32'))
-        episode_data['terminals'].append(done)
-        if done:
-            break
-
-for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:
-    episode_data[k] = np.stack(episode_data[k])
-with open('/home/kjlin0508/Course_work/AI_Intro/RL_for_Quatitatitive_Trading/Trajectory_Transformer/trajectory/datasets/Medium/stock_{}'.format(quat_type) + '.pkl', 'wb') as f:
-    pickle.dump(episode_data, f)
\ No newline at end of file
diff --git a/Trajectory_Transformer/trajectory/datasets/sequence.py b/Trajectory_Transformer/trajectory/datasets/sequence.py
index 3c3bb2b..49041a2 100644
--- a/Trajectory_Transformer/trajectory/datasets/sequence.py
+++ b/Trajectory_Transformer/trajectory/datasets/sequence.py
@@ -62,12 +62,12 @@ class SequenceDataset(torch.utils.data.Dataset):
         #     print(f'[ datasets/sequence ] Modifying environment')
         #     dataset = preprocess_fn(dataset)
         ##
-        observations = dataset['observations']
-        actions = dataset['actions']
-        next_observations = dataset['next_observations']
-        rewards = dataset['rewards']
-        terminals = dataset['terminals']
-        realterminals = [False]*len(dataset['terminals'])
+        observations = np.array(dataset['observations'])
+        actions = np.array(dataset['actions'])
+        next_observations = np.array(dataset['next_observations'])
+        rewards = np.array(dataset['rewards'])
+        terminals = np.array(dataset['terminals'])
+        realterminals = np.array(dataset['terminals'])
 
         self.observations_raw = observations
         self.actions_raw = actions
diff --git a/logs/stock_2330/gpt/azure/state_0.pt b/logs/stock_2330/gpt/azure/state_0.pt
index 215a98c..2008ae4 100644
Binary files a/logs/stock_2330/gpt/azure/state_0.pt and b/logs/stock_2330/gpt/azure/state_0.pt differ